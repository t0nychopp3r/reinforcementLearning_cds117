{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning - Resource Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kudos to:\n",
    "\n",
    "https://www.gymlibrary.dev/content/environment_creation/\n",
    "\n",
    "https://www.youtube.com/watch?v=bD6V3rcr_54&ab_channel=NicholasRenotte \n",
    "\n",
    "Version 1.0:\n",
    "\n",
    "- Added negative Reward when moving around, Maximum of 100 Steps possible, then set Episode to done\n",
    "- Add a Reward if near the Target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Gym Environment for Resource Manager\n",
    "#The environment is a 2D grid with 4 possible actions: up, down, left, right\n",
    "#The agent can move in any direction but cannot move outside the grid\n",
    "\n",
    "class ResourceManagerEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "\n",
    "    def __init__(self, grid_size=10, render_mode=None):\n",
    "\n",
    "        #initialize the reward\n",
    "        self.total_reward = 0\n",
    "\n",
    "        #Define Grid Size\n",
    "        self.grid_size = grid_size\n",
    "        self.window_size = 500\n",
    "\n",
    "        #Action Space:\n",
    "        #0: Right, 1: up, 2: left, 3: down\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        #Map the action to the corresponding movement\n",
    "        self.action_to_direction = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, 1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "        #Observation Space:\n",
    "        #The observation space is a 2D grid with the agent's position marked as 1\n",
    "        #and the rest of the grid marked as 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(grid_size, grid_size), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "\n",
    "\n",
    "    #Needed for Environment Reset\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        #Choose the agent's initial position at random\n",
    "        self.agent_position = self.np_random.integers(low=0, high=self.grid_size, size=(2,))\n",
    "\n",
    "        #Set the target position at random until it is different from the agent's position\n",
    "        self.target_position = self.agent_position\n",
    "        while np.all(self.target_position == self.agent_position):\n",
    "            self.target_position = self.np_random.integers(low=0, high=self.grid_size, size=(2,))\n",
    "\n",
    "        self.total_reward = 0\n",
    "\n",
    "        observation = self.get_obs()\n",
    "        info = self.get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render_frame()\n",
    "\n",
    "        return observation, info\n",
    "    \n",
    "    def get_obs(self):\n",
    "        #Initialize observation\n",
    "        observation = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)\n",
    "\n",
    "        #Mark the agent's position\n",
    "        observation[tuple(self.agent_position)] = 1\n",
    "        return observation\n",
    "    \n",
    "    def get_info(self):\n",
    "        #Initialize info\n",
    "        info = {\n",
    "            'agent_position': self.agent_position,\n",
    "            'target_position': self.target_position,\n",
    "            'total_reward': self.total_reward\n",
    "        }\n",
    "        return info\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        # ***** Move around the grid *****\n",
    "\n",
    "        #store the agent's position before taking a step\n",
    "        original_position = np.copy(self.agent_position)\n",
    "        #choose a direction\n",
    "        direction = self.action_to_direction[action]\n",
    "        #Move the agent in that direction\n",
    "        self.agent_position = np.clip(\n",
    "            self.agent_position + direction,\n",
    "            0,\n",
    "            self.grid_size - 1\n",
    "        )\n",
    "        #check if the agent's position has changed\n",
    "        position_changed = not np.all(self.agent_position == original_position)\n",
    "\n",
    "        #define when done\n",
    "        done = np.all(self.agent_position == self.target_position)\n",
    "\n",
    "        # ***** Reward Function *****\n",
    "\n",
    "        #calculate Manhatten distance between agent and target\n",
    "        distance_to_target = np.abs(self.agent_position[0] - self.target_position[0]) + np.abs(self.agent_position[1] - self.target_position[1])\n",
    "\n",
    "\n",
    "        if done:\n",
    "            reward = 0  #the agent has reached the target\n",
    "        elif position_changed:\n",
    "            reward = -1  #the agent has taken a step\n",
    "        else:\n",
    "            #idea: implement already a reward if the agent did not move\n",
    "            reward = -10  #the agent didn't move, so give a -10 reward\n",
    "        \n",
    "        #give reward to the agent if it is close to the target\n",
    "        if distance_to_target == 1:\n",
    "            reward += 2  # +2 reward when immediately around the target\n",
    "        elif distance_to_target <= 4:\n",
    "            reward += 1  # +1 reward when within 16 fields around the target\n",
    "\n",
    "        self.total_reward += reward\n",
    "\n",
    "        #stop the episode when reward is -100\n",
    "        if self.total_reward == -100:\n",
    "            done = True\n",
    "            self.reset()\n",
    "\n",
    "\n",
    "        observation = self.get_obs()\n",
    "        info = self.get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render_frame()\n",
    "\n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "            if self.render_mode == \"rgb_array\":\n",
    "                return self.render_frame()\n",
    "    \n",
    "    def render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.grid_size\n",
    "        )  # The size of a single grid square in pixels\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self.target_position,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self.agent_position + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.grid_size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "        \n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "        self.window = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "register(\n",
    "    id='Resource-Manager-v10',\n",
    "    entry_point='ResourceManager_v1_0.ipynb:ResourceManagerEnv',\n",
    "    max_episode_steps=300,\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "\n",
    "env = ResourceManagerEnv(grid_size=grid_size, render_mode=\"human\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent position: [8 8], Target position: [7 4], Total reward: -1, Step reward: -1, Episode: 0\n",
      "Agent position: [8 9], Target position: [7 4], Total reward: -2, Step reward: -1, Episode: 0\n",
      "Agent position: [8 9], Target position: [7 4], Total reward: -12, Step reward: -10, Episode: 0\n",
      "Agent position: [7 9], Target position: [7 4], Total reward: -13, Step reward: -1, Episode: 0\n",
      "Agent position: [7 8], Target position: [7 4], Total reward: -13, Step reward: 0, Episode: 0\n",
      "Agent position: [7 7], Target position: [7 4], Total reward: -13, Step reward: 0, Episode: 0\n",
      "Agent position: [6 7], Target position: [7 4], Total reward: -13, Step reward: 0, Episode: 0\n",
      "Agent position: [6 8], Target position: [7 4], Total reward: -14, Step reward: -1, Episode: 0\n",
      "Agent position: [6 7], Target position: [7 4], Total reward: -14, Step reward: 0, Episode: 0\n",
      "Agent position: [5 7], Target position: [7 4], Total reward: -15, Step reward: -1, Episode: 0\n",
      "Agent position: [4 7], Target position: [7 4], Total reward: -16, Step reward: -1, Episode: 0\n",
      "Agent position: [4 8], Target position: [7 4], Total reward: -17, Step reward: -1, Episode: 0\n",
      "Agent position: [5 8], Target position: [7 4], Total reward: -18, Step reward: -1, Episode: 0\n",
      "Agent position: [4 8], Target position: [7 4], Total reward: -19, Step reward: -1, Episode: 0\n",
      "Agent position: [3 8], Target position: [7 4], Total reward: -20, Step reward: -1, Episode: 0\n",
      "Agent position: [2 8], Target position: [7 4], Total reward: -21, Step reward: -1, Episode: 0\n",
      "Agent position: [2 7], Target position: [7 4], Total reward: -22, Step reward: -1, Episode: 0\n",
      "Agent position: [2 6], Target position: [7 4], Total reward: -23, Step reward: -1, Episode: 0\n",
      "Agent position: [1 6], Target position: [7 4], Total reward: -24, Step reward: -1, Episode: 0\n",
      "Agent position: [1 5], Target position: [7 4], Total reward: -25, Step reward: -1, Episode: 0\n",
      "Agent position: [0 5], Target position: [7 4], Total reward: -26, Step reward: -1, Episode: 0\n",
      "Agent position: [0 6], Target position: [7 4], Total reward: -27, Step reward: -1, Episode: 0\n",
      "Agent position: [1 6], Target position: [7 4], Total reward: -28, Step reward: -1, Episode: 0\n",
      "Agent position: [1 5], Target position: [7 4], Total reward: -29, Step reward: -1, Episode: 0\n",
      "Agent position: [1 4], Target position: [7 4], Total reward: -30, Step reward: -1, Episode: 0\n",
      "Agent position: [1 5], Target position: [7 4], Total reward: -31, Step reward: -1, Episode: 0\n",
      "Agent position: [1 6], Target position: [7 4], Total reward: -32, Step reward: -1, Episode: 0\n",
      "Agent position: [0 6], Target position: [7 4], Total reward: -33, Step reward: -1, Episode: 0\n",
      "Agent position: [1 6], Target position: [7 4], Total reward: -34, Step reward: -1, Episode: 0\n",
      "Agent position: [1 5], Target position: [7 4], Total reward: -35, Step reward: -1, Episode: 0\n",
      "Agent position: [2 5], Target position: [7 4], Total reward: -36, Step reward: -1, Episode: 0\n",
      "Agent position: [1 5], Target position: [7 4], Total reward: -37, Step reward: -1, Episode: 0\n",
      "Agent position: [2 5], Target position: [7 4], Total reward: -38, Step reward: -1, Episode: 0\n",
      "Agent position: [2 4], Target position: [7 4], Total reward: -39, Step reward: -1, Episode: 0\n",
      "Agent position: [1 4], Target position: [7 4], Total reward: -40, Step reward: -1, Episode: 0\n",
      "Agent position: [1 3], Target position: [7 4], Total reward: -41, Step reward: -1, Episode: 0\n",
      "Agent position: [1 2], Target position: [7 4], Total reward: -42, Step reward: -1, Episode: 0\n",
      "Agent position: [1 1], Target position: [7 4], Total reward: -43, Step reward: -1, Episode: 0\n",
      "Agent position: [1 0], Target position: [7 4], Total reward: -44, Step reward: -1, Episode: 0\n",
      "Agent position: [1 1], Target position: [7 4], Total reward: -45, Step reward: -1, Episode: 0\n",
      "Agent position: [2 1], Target position: [7 4], Total reward: -46, Step reward: -1, Episode: 0\n",
      "Agent position: [3 1], Target position: [7 4], Total reward: -47, Step reward: -1, Episode: 0\n",
      "Agent position: [3 2], Target position: [7 4], Total reward: -48, Step reward: -1, Episode: 0\n",
      "Agent position: [2 2], Target position: [7 4], Total reward: -49, Step reward: -1, Episode: 0\n",
      "Agent position: [2 3], Target position: [7 4], Total reward: -50, Step reward: -1, Episode: 0\n",
      "Agent position: [1 3], Target position: [7 4], Total reward: -51, Step reward: -1, Episode: 0\n",
      "Agent position: [1 4], Target position: [7 4], Total reward: -52, Step reward: -1, Episode: 0\n",
      "Agent position: [1 3], Target position: [7 4], Total reward: -53, Step reward: -1, Episode: 0\n",
      "Agent position: [2 3], Target position: [7 4], Total reward: -54, Step reward: -1, Episode: 0\n",
      "Agent position: [2 4], Target position: [7 4], Total reward: -55, Step reward: -1, Episode: 0\n",
      "Agent position: [2 5], Target position: [7 4], Total reward: -56, Step reward: -1, Episode: 0\n",
      "Agent position: [3 5], Target position: [7 4], Total reward: -57, Step reward: -1, Episode: 0\n",
      "Agent position: [3 6], Target position: [7 4], Total reward: -58, Step reward: -1, Episode: 0\n",
      "Agent position: [2 6], Target position: [7 4], Total reward: -59, Step reward: -1, Episode: 0\n",
      "Agent position: [2 7], Target position: [7 4], Total reward: -60, Step reward: -1, Episode: 0\n",
      "Agent position: [2 6], Target position: [7 4], Total reward: -61, Step reward: -1, Episode: 0\n",
      "Agent position: [3 6], Target position: [7 4], Total reward: -62, Step reward: -1, Episode: 0\n",
      "Agent position: [3 5], Target position: [7 4], Total reward: -63, Step reward: -1, Episode: 0\n",
      "Agent position: [3 4], Target position: [7 4], Total reward: -63, Step reward: 0, Episode: 0\n",
      "Agent position: [2 4], Target position: [7 4], Total reward: -64, Step reward: -1, Episode: 0\n",
      "Agent position: [3 4], Target position: [7 4], Total reward: -64, Step reward: 0, Episode: 0\n",
      "Agent position: [2 4], Target position: [7 4], Total reward: -65, Step reward: -1, Episode: 0\n",
      "Agent position: [2 5], Target position: [7 4], Total reward: -66, Step reward: -1, Episode: 0\n",
      "Agent position: [3 5], Target position: [7 4], Total reward: -67, Step reward: -1, Episode: 0\n",
      "Agent position: [3 4], Target position: [7 4], Total reward: -67, Step reward: 0, Episode: 0\n",
      "Agent position: [2 4], Target position: [7 4], Total reward: -68, Step reward: -1, Episode: 0\n",
      "Agent position: [2 5], Target position: [7 4], Total reward: -69, Step reward: -1, Episode: 0\n",
      "Agent position: [2 6], Target position: [7 4], Total reward: -70, Step reward: -1, Episode: 0\n",
      "Agent position: [2 5], Target position: [7 4], Total reward: -71, Step reward: -1, Episode: 0\n",
      "Agent position: [1 5], Target position: [7 4], Total reward: -72, Step reward: -1, Episode: 0\n",
      "Agent position: [2 5], Target position: [7 4], Total reward: -73, Step reward: -1, Episode: 0\n",
      "Agent position: [3 5], Target position: [7 4], Total reward: -74, Step reward: -1, Episode: 0\n",
      "Agent position: [4 5], Target position: [7 4], Total reward: -74, Step reward: 0, Episode: 0\n",
      "Agent position: [4 6], Target position: [7 4], Total reward: -75, Step reward: -1, Episode: 0\n",
      "Agent position: [4 5], Target position: [7 4], Total reward: -75, Step reward: 0, Episode: 0\n",
      "Agent position: [4 6], Target position: [7 4], Total reward: -76, Step reward: -1, Episode: 0\n",
      "Agent position: [3 6], Target position: [7 4], Total reward: -77, Step reward: -1, Episode: 0\n",
      "Agent position: [3 7], Target position: [7 4], Total reward: -78, Step reward: -1, Episode: 0\n",
      "Agent position: [3 8], Target position: [7 4], Total reward: -79, Step reward: -1, Episode: 0\n",
      "Agent position: [3 9], Target position: [7 4], Total reward: -80, Step reward: -1, Episode: 0\n",
      "Agent position: [4 9], Target position: [7 4], Total reward: -81, Step reward: -1, Episode: 0\n",
      "Agent position: [5 9], Target position: [7 4], Total reward: -82, Step reward: -1, Episode: 0\n",
      "Agent position: [6 9], Target position: [7 4], Total reward: -83, Step reward: -1, Episode: 0\n",
      "Agent position: [5 9], Target position: [7 4], Total reward: -84, Step reward: -1, Episode: 0\n",
      "Agent position: [5 8], Target position: [7 4], Total reward: -85, Step reward: -1, Episode: 0\n",
      "Agent position: [5 7], Target position: [7 4], Total reward: -86, Step reward: -1, Episode: 0\n",
      "Agent position: [5 8], Target position: [7 4], Total reward: -87, Step reward: -1, Episode: 0\n",
      "Agent position: [5 9], Target position: [7 4], Total reward: -88, Step reward: -1, Episode: 0\n",
      "Agent position: [6 9], Target position: [7 4], Total reward: -89, Step reward: -1, Episode: 0\n",
      "Agent position: [5 9], Target position: [7 4], Total reward: -90, Step reward: -1, Episode: 0\n",
      "Agent position: [4 9], Target position: [7 4], Total reward: -91, Step reward: -1, Episode: 0\n",
      "Agent position: [4 8], Target position: [7 4], Total reward: -92, Step reward: -1, Episode: 0\n",
      "Agent position: [5 8], Target position: [7 4], Total reward: -93, Step reward: -1, Episode: 0\n",
      "Agent position: [5 7], Target position: [7 4], Total reward: -94, Step reward: -1, Episode: 0\n",
      "Agent position: [6 7], Target position: [7 4], Total reward: -94, Step reward: 0, Episode: 0\n",
      "Agent position: [5 7], Target position: [7 4], Total reward: -95, Step reward: -1, Episode: 0\n",
      "Agent position: [6 7], Target position: [7 4], Total reward: -95, Step reward: 0, Episode: 0\n",
      "Agent position: [5 7], Target position: [7 4], Total reward: -96, Step reward: -1, Episode: 0\n",
      "Agent position: [6 7], Target position: [7 4], Total reward: -96, Step reward: 0, Episode: 0\n",
      "Agent position: [6 8], Target position: [7 4], Total reward: -97, Step reward: -1, Episode: 0\n",
      "Agent position: [6 9], Target position: [7 4], Total reward: -98, Step reward: -1, Episode: 0\n",
      "Agent position: [5 9], Target position: [7 4], Total reward: -99, Step reward: -1, Episode: 0\n",
      "Agent position: [1 3], Target position: [2 8], Total reward: 0, Step reward: -1, Episode: 0\n",
      "Agent position: [8 5], Target position: [7 8], Total reward: 0, Step reward: 0, Episode: 1\n",
      "Agent position: [8 4], Target position: [7 8], Total reward: -1, Step reward: -1, Episode: 1\n",
      "Agent position: [9 4], Target position: [7 8], Total reward: -2, Step reward: -1, Episode: 1\n",
      "Agent position: [9 5], Target position: [7 8], Total reward: -3, Step reward: -1, Episode: 1\n",
      "Agent position: [8 5], Target position: [7 8], Total reward: -3, Step reward: 0, Episode: 1\n",
      "Agent position: [9 5], Target position: [7 8], Total reward: -4, Step reward: -1, Episode: 1\n",
      "Agent position: [9 4], Target position: [7 8], Total reward: -5, Step reward: -1, Episode: 1\n",
      "Agent position: [9 5], Target position: [7 8], Total reward: -6, Step reward: -1, Episode: 1\n",
      "Agent position: [8 5], Target position: [7 8], Total reward: -6, Step reward: 0, Episode: 1\n",
      "Agent position: [8 4], Target position: [7 8], Total reward: -7, Step reward: -1, Episode: 1\n",
      "Agent position: [9 4], Target position: [7 8], Total reward: -8, Step reward: -1, Episode: 1\n",
      "Agent position: [9 5], Target position: [7 8], Total reward: -9, Step reward: -1, Episode: 1\n",
      "Agent position: [9 6], Target position: [7 8], Total reward: -9, Step reward: 0, Episode: 1\n",
      "Agent position: [9 7], Target position: [7 8], Total reward: -9, Step reward: 0, Episode: 1\n",
      "Agent position: [8 7], Target position: [7 8], Total reward: -9, Step reward: 0, Episode: 1\n",
      "Agent position: [8 8], Target position: [7 8], Total reward: -8, Step reward: 1, Episode: 1\n",
      "Agent position: [8 9], Target position: [7 8], Total reward: -8, Step reward: 0, Episode: 1\n",
      "Agent position: [7 9], Target position: [7 8], Total reward: -7, Step reward: 1, Episode: 1\n",
      "Agent position: [7 9], Target position: [7 8], Total reward: -15, Step reward: -8, Episode: 1\n",
      "Agent position: [7 8], Target position: [7 8], Total reward: -14, Step reward: 1, Episode: 1\n"
     ]
    }
   ],
   "source": [
    "#Run the enfironment for 20 episodes\n",
    "episodes = 2\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        print(f\"Agent position: {info['agent_position']}, Target position: {info['target_position']}, Total reward: {info['total_reward']}, Step reward: {reward}, Episode: {episode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
